{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Nanodegree Udacity - Capstone Project - Data Engineering \n",
    "### Building a Data Warehouse to analyse immigration, climate, and demographic data\n",
    "\n",
    "#### Project Summary\n",
    "The key subject of this project iss to create an ETL pipeline for I94 immigration, global land temperatures and US demographics datasets to form an analytics database on immigration events. A use case for this analytics database is to find immigration patterns. The main element is to enable a [singe-point-of-truth architecure](https://en.wikipedia.org/wiki/Single_source_of_truth) based on the data warehouse.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libaries\n",
    "\n",
    "Section to import all relevant libaries and to inalize the installations for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go \n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import utility\n",
    "import etl_functions\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utility)\n",
    "from utility import visualize_missng_values, clean_immigration, clean_temperature_data\n",
    "from utility import clean_demographics_data, print_formatted_float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Data - Loading Process\n",
    "\n",
    "Loading all access keys, which are necessary to initiate the AWS infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('Capstone Config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the SPARK session\n",
    "\n",
    "The SPARK session has to be initiated to start the large-scale data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enbableHiveSupport().getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope of the project\n",
    "\n",
    "This capstone project will focus on three major datasets which will be used to create a data warehouse including respective fact and dimension tables.\n",
    "\n",
    "* Used Datasets\n",
    "\n",
    "    1. [US Cities: Demographics](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "    2. [Climate Change: Earth Surface Temperature Data](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "    3. [I94 Immigration Data](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "\n",
    "* Tools to analyze the data\n",
    "\n",
    "    1. Python: as programming language - especially to process the data\n",
    "    2. PySpark: used for large datasets to process the data\n",
    "    3. Pandas: data analysis for small data sets\n",
    "    4. AWS S3: used for plain data storage\n",
    "    5. AWS Redshift: used for data warehousing and data analysis\n",
    "\n",
    "### Collect the relevant data\n",
    "\n",
    "The follwoing data sets are recommended and given to fulfil the project goal:\n",
    "\n",
    "| Data Set | Format / Data Type | Description |\n",
    "| ---      | ---                | ---         |\n",
    "| [US Cities: Demographics](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) | CSV (Comma Separated Value) | This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey. |\n",
    "| [Climate Change: Earth Surface Temperature Data](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data) | CSV (Comma Separated Value) | Provided by Kaggle, this data set points out the average temperature of significant cities and metropoles all over the world. |\n",
    "| [I94 Immigration Data](https://travel.trade.gov/research/reports/i94/historical/2016.html) | SAS (Statistical Analysis Software) | The US Immigration Center provides information about the arrivals of international travelers focussing on different global regions and countries. Furthermore, it shows data about the visa type, transportation mode, groups of age, visited states, and the top ports of entry. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the relevant data - I94 Immigration\n",
    "\n",
    "This data comes from the US National Tourism and Trade Office. In the past all foreign visitors to the U.S. arriving via air or sea were required to complete paper Customs and Border Protection Form I-94 Arrival/Departure Record or Form I-94W Nonimmigrant Visa Waiver Arrival/Departure Record and this dataset comes from this forms.\n",
    "\n",
    "*I94 Immigration data* is the foundation of the data warehouse and is structuted as a repository for all customers. The dataset is strcutured for the year 2016 and is devided by month. The folder is located at `../../data/18-83510-I94-Data-2016`. Based on the month the data is stoted in a SAS binary format called *sas7bdat*.\n",
    "\n",
    "The following step is loading the revelant data for *April 2016*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(fname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Showing first fives rows of dataset I94 Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df.limit(5).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Number of Records\n",
    "\n",
    "Counting the total number of loaded records in the data warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_formatted_float(immigration_df.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Drop Duplicates\n",
    "\n",
    "Focussing on the attribute *visapost*, showing the first 10 rows of filtered duplpicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df.select(\"visapost\").dropDuplicates().show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for dataset - I94 Immigration\n",
    "\n",
    "[Data Dictionary I94 Immigration](https://github.com/KCvW/DataEng/blob/main/Capstone/Data%20Dictionary%20I94%20Immigration.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the relevant data - World Temperature Data\n",
    "\n",
    "This dataset is provided by *Kaggle* and shows the temperature for city around the globe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading world temperature data\n",
    "file_name = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature_df = spark.read.csv(file_name, header=True, inferSchema=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Showing the first ten rows of dataset World Temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df.limit(10).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for dataset - World Temperature\n",
    "\n",
    "[Data Dictionary World Temperature](https://github.com/KCvW/DataEng/blob/main/Capstone/Data%20Dictionary%20World%20Temperature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Number of Records\n",
    "\n",
    "Counting the total number of records loaded in the data warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the total count of records loaded\n",
    "print_formatted_float(temperature_df.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the relevant data - U.S. City Demographic\n",
    "\n",
    "*OpenSoft* is providing this dataset. It highlights information about demographic details of U.S. cities and census-deignated places. The threshold in regards to the population is equal or greater to 65,000 citizen. The raw data is provided by the U.S. Census Bureau based on the 2015 American Community Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "file_name = \"us-cities-demographics.csv\"\n",
    "demographics_df = spark.read.csv(file_name inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Showing the first ten rows of dataset U.S. City Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df.limit(10).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for dataset - U.S. City Demographic\n",
    "\n",
    "[U.S. City Demographic](https://github.com/KCvW/DataEng/blob/main/Capstone/Data%20Dictionary%20US%20City%20Demographic.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Number of Records\n",
    "\n",
    "Counting the total number of records loaded into the data warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the total count of records loaded\n",
    "print_formatted_float(demographics_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# not needed - implemented in a further cell\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data - Immigration Data\n",
    "Exploration and data analysis to check the quality of the raw data sets.\n",
    "Starting with `Immigration Data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# focussing on a monthly basis all files are listed\n",
    "files = os.listdir('../../data/18-83510-I94-Data-2016')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data schema\n",
    "immigration_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Check - Graphical analysis concerning missing data (Immigration Data)\n",
    "Listing all columns of the data schema to visualze the percentage of missing data per attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.visualize_missing_values_spark(immigration_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Process\n",
    "On  scope are all columns and rows with missing values equal or higher than 90%, because these ones do not contain sufficient data for an reliable analysis approach:\n",
    "\n",
    "* Deleting all columns with a missing rate greater than 90%\n",
    "* Deleting all rows with a missing rate of 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all columns with insufficent data quality and values\n",
    "cols = ['occup', 'entdepu', 'insnum']\n",
    "new_immi_df = immigration_df.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new data schema after deletion process\n",
    "new_immi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all duplicates\n",
    "new_immi_df = new_immi_df.dropDuplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all entries\n",
    "print_formatted_float(new_immi_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all rows with insufficient data quality and values - no entries\n",
    "new_immi_df = new_immi_df.dropna(how = 'all', subject = [('cicid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count after deletion process\n",
    "print_formatted_float(new_immi_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immigration dataframe after cleansing\n",
    "new_immigration_df = utility.clean_spark_immigration_data(immigration_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data - World Temperature Data\n",
    "Exploration and data analysis to check the quality of the raw data sets.\n",
    "Continuing with `World Temperature Data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show initial data schema\n",
    "temperature_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Check - Graphical analysis concerning missing data (World Temperature Data)\n",
    "Listing all columns of the data schema to visualze the percentage of missing data per attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column type dt changed to string\n",
    "temperature_df2 = temperature_df.withColumn(\"dt\", col(\"dt\").cast(StringType()))\n",
    "utility.visualize_missing_values_spark(temperature_df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to clean the relevant data\n",
    "\n",
    "To focus on a relibale dataset it is again necessary to purge the data based on the respective values:\n",
    "\n",
    "* Deletion of rows which have a missing value\n",
    "* Detection of rows with duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process of cleansing the data\n",
    "new_temperture_df = utility.clean_spark_temperature_data(temperature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed as other procedures are taking care of data quality\n",
    "\"\"\"\n",
    "# show columns with insufficient data\n",
    "utility.visualize_missing_values_spark(temperature_df2)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data (U.S. City Demographic Data)\n",
    "\n",
    "The scope is on the raw data set to evalute the quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of records in the raw data set\n",
    "print_formatted_float(demographics_df.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the data schema\n",
    "demographics_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing missing values in the raw dataset\n",
    "\n",
    "visualization of the raw datasets to get an impression of the entore data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the missing attributes\n",
    "utility.visualize_missing_values_spark(demographics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing columns with no or missing data\n",
    "nulls_df = pd.DataFrame(data=demographics_df.toPandas().isnull().sum(), columns=['values'])\n",
    "nulls_df = nulls_df.reset_index()\n",
    "nulls_df.columns = ['cols', 'values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the missing in percent (%)\n",
    "nulls_df['% missing value'] = 100*nulls_df['values']/demographics_df.count()\n",
    "nulls_df[nulls_df['% missing values']>0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further steps to come to a clean dataset\n",
    "\n",
    "As mentioned above, also this dataset needs a deepdive to focus on reliable raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process of cleaning the dataset\n",
    "new_demographics_df = utility.clean_spark_demographics_data(demographics_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "Coming from a analysis project it is essential to point out the core data model.\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The conceputal data model this is core to define the main fact table and all related dimension tables:\n",
    "\n",
    "[Conceptual Data Model](https://github.com/KCvW/DataEng/blob/main/Capstone/Capstone%20Data%20Model%20(conceptual).png)\n",
    "\n",
    "Coming from this data model it is key to understand how to combine the data to realize relibale and sophisticated analysis approaches. Adapted from the conceptual model the following structure is crucial for the capstone project.\n",
    "\n",
    "The dimension table `country` combines data reflecting the global city temperatures and information from the U.S. immigration center. Therefore it is possible to calculate statistical combinations liek correlations between both dimensions. \n",
    "\n",
    "A similar approach is given for the dimension table `usa_demographics`, which links its own data with the fact table `immigration`. Hence it is possible to extract information about certain immigration pattern such as the combination of U.S. demographics and the population in defined U.S. states. So it is feasible to check whether bigger or ever proser U.S. states attract more poeple in a given time frame. Another interesting approach is to focus on per-defined classes like age, race or even sex. All of these attributes can be visualized in a dashboard to drill down the details or to condens the information. A combination of this data might be helpful for immigration and tourism offices to define strategies for the future.\n",
    "\n",
    "Another linkage is given by the dimension table `visa_type`. Referring to the primary key `visa_type_key` this table is directly connected to the fact table `immigration` whereas the data is provided by the immigration data set.\n",
    "\n",
    "As highlighted in the picture `Conceptual Data Model` the fact table `immigration` is the core of the entire data model. All data is provided by the immigration data set and is linked in multiple ways to the mentioned dimension tables. The meta data which is defining the structure is described in the data dictionary.\n",
    "\n",
    "#### 3.2 Data Pipelines & Data Mapping\n",
    "\n",
    "In this sub-chapter the scope is on the desciptrion how to create the before-mentioned tables based on the data pipelines.\n",
    "The primary steps are:\n",
    "\n",
    "1. Initial load of the entire data sets\n",
    "2. Process of cleaning `I94 Immigration` - needed to create a monthly data frame in Apache SPARK\n",
    "3. Creating dimension tables - `visa_type` and `immi_calendar`\n",
    "4. Extraction and process of cleaning of `World Temperature Data`\n",
    "5. Create further tables - dimension table `country` and fact table `immigration`\n",
    "6. Loasding and cleansing the `Demographic` data\n",
    "7. Creating the dimension table `usa_demographics`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "The function in the code below creates the dimension table `immi_calendar` adapted from the arrival date. In detail:\n",
    "\n",
    "* `param df` - Dataframe in Spark listing all immigration events\n",
    "* `param output_data`- path to where the dimensional dataframe is written to\n",
    "* `return` - Dataframe in Spark showing the dimensional table `immi_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating the dimension table immi_calendar\n",
    "def create_immigration_calendar_dimension(df, output_data):\n",
    "    \n",
    "    # Conversion of arrival date in SAS format to a common datetime object (utf)\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # Creation of data frame for calenmdar based on column arrdate\n",
    "    calendar_df = df.select(['arrdate']).withColumn(\"arrdate\", get_datetime(df.arrdate)).distinct()\n",
    "    \n",
    "    # Extension of data frame by adding further columns\n",
    "    calendar_df = calendar_df.withColumn('arrival_day', dayofmonth('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_week', weekofyear('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_month', month('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_year', year('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_weekday', dayofweek('arrdate'))\n",
    "    \n",
    "    # Creation of an ID in the dataframe calendar\n",
    "    calendar_df = calendar_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    # Write the dimensional table to Parquet\n",
    "    partition_columns = ['arrival_year', 'arrival_month', 'arrival_week']\n",
    "    calendar_df.write.parquet(output_data + \"immigration_calendar\", partitionBy=partition_columns, mode=\"overwrite\")\n",
    "    \n",
    "    return calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"tables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = create_immigration_calendar_dimension(new_immigration_df, output_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the dimension table `country` is created.\n",
    "Similar to the other dimension table the data is merged from `immigration` and `global land temperature` data\n",
    "\n",
    "* `param df` - Dataframe in Spark listing all immigration events\n",
    "* `temp df` - Dataframe in Spark listing the global land tempearture data\n",
    "* `param output_data`- path to where the dimensional dataframe is written to\n",
    "* `return` - Dataframe in Spark showing the dimensional table `immi_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_dimension_table(df, temp_df, output_data):\n",
    "    \n",
    "    # Pulling the temperature data in an aggregated way\n",
    "    agg_temp = utility.aggregate_temperature_data(temp_df).toPandas()\n",
    "    \n",
    "    # Loading the files i94.res.csv to map the country data\n",
    "    mapping_codes = pd.read_csv('i94res.csv')\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_country_average_temperature(name):\n",
    "        print(\"Processing: \", name)\n",
    "        avg_temp = agg_temp[agg_temp['Country']==name]['average_temperature']\n",
    "        \n",
    "        if not avg_temp.empty:\n",
    "            return str(avg_temp.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @udf()\n",
    "    def get_country_name(code):\n",
    "        name = mapping_codes[mapping_codes['code']==code]['Name'].iloc[0]\n",
    "        \n",
    "        if name:\n",
    "            return name.title()\n",
    "        return None\n",
    "    \n",
    "    # selection of i94res columns and renaming these ones\n",
    "    dim_df = df.select(['i94res']).distinct().withColumnRenamed('i94res', 'country_code')\n",
    "    \n",
    "    # Creation of column country_name\n",
    "    dim_df = dim_df.withColumn('country_name', get_country_name(dim_df.country_code))\n",
    "    \n",
    "    # Creation of column average_temperature\n",
    "    dim_df = dim_df.withColumn('average_temperature', get_country_average_temperature(dim_df.country_name))\n",
    "    \n",
    "    # Write the tables to Parquet\n",
    "    dim_df.write.parquet(output_data + \"country\", mode=\"overwrite\")\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dim_f = create_country_dimension_table(new_immigration_df, new_temperture_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dim_f.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create the dimension table `visa_type`, which is done in the same manner.\n",
    "\n",
    "* `param df` - Dataframe in Spark listing all immigration events\n",
    "* `param output_data`- path to where the dimensional dataframe is written to\n",
    "* `return` - Dataframe in Spark showing the dimensional table `immi_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visa_type_dimension_table(df, output_data):\n",
    "    \n",
    "    # Creating the dataframe from the column visatype\n",
    "    visatype_df = df.select(['visatype']).distint()\n",
    "    \n",
    "    # Adding a column for the id\n",
    "    visatype_df = visatype_df.withColumn('visa_type_key', monotonically_increasing_id())\n",
    "    \n",
    "    # Write the tables to Parquet\n",
    "    visatype_df-write-parquet(output_data + \"visa_type\", mode=\"overwrite\")\n",
    "    \n",
    "    return visatype_df\n",
    "\n",
    "def get_visa_type_dimension(output_data):\n",
    "    return spark.read.parquet(output_data + \"visa_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test function for dimension table visa_type\n",
    "visatype_df = create_visa_type_dimension_table(new_immigration_df, output_data)\n",
    "visatype_df.show(n=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the demographics table is created, which is the last dimension table based on the data model. Referring to the common schema the function is nearly the same:\n",
    "\n",
    "* `param df` - Dataframe in Spark listing all immigration events\n",
    "* `param output_data`- path to where the dimensional dataframe is written to\n",
    "* `return` - Dataframe in Spark showing the dimensional table `usa-demographics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographics_dimension_table(df, output_data):\n",
    "    dim_df =df.withColumnRenamed('Median Age','median_age')\\\n",
    "            .withColumnRenamed('Male Population', 'male_population')\\\n",
    "            .withColumnRenamed('Female Population', 'femaile_population')\\\n",
    "            .withColumnRenamed('Total Population', 'total_population')\\\n",
    "            .withColumnRenamed('Number of Veterans', 'number_of_veterans')\\\n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born')\\\n",
    "            .withColumnRenames('Average Household Size', 'average_household_size')\\\n",
    "            .withColumnRenamed('State Code', 'state_code')\n",
    "# Adding a column for the ID\n",
    "    dim_df = dim_df.withColumn('id', monotonically_increasing_id())\n",
    "# write tables to Parquet\n",
    "    dim_df.write.parquet(output_data + \"demographics\", mode=\"overwrite\")\n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dim_df = create_demographics_dimension_table(new_demographics_df, output_data)\n",
    "demographics_dim_df.limit(5).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the fact table `Immigration` has to be initiated. Also this element is created in a similar manner.\n",
    "\n",
    "* `param df` - Dataframe in Spark listing all immigration events\n",
    "* `visa_type_df` - refers to the Spark dataframe listing all the global city temperature data\n",
    "* `param output_data`- path to where the dimensional dataframe is written to\n",
    "* `return` - Dataframe in Spark showing the dimensional table `immi-calendar`\n",
    "\n",
    "In addition a user defined function is used to get the visa key.\n",
    "\n",
    "* `param visa_type` - visa type U.S. non-immigrant\n",
    "* `return` - respective visa key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_immigration_fact_table(df, output_data):\n",
    "    #getting dimension visa_type\n",
    "    dim_df = get_visa_type_dimension(output_data).toPandas()\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_visa_key(visa_type):\n",
    "        key_series = dim_df[dim_df['visatype']==visa_type]['visa_type_key']\n",
    "        \n",
    "        if not key_series.empty:\n",
    "            return str(key_series.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Based on udf the arrivial date is converted from SAS to a datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # Referring to data model certain columns are renamed\n",
    "    df = df.withColumnRenamed('cicid', 'record_id') \\\n",
    "            .withColumnRenamed('i94res', 'country_residence_code') \\\n",
    "            .withColumnRenamed('i94addr', 'state_code')\n",
    "            \n",
    "    # create key visa_type\n",
    "    df = df.withColumn('visa_type_key', get_visa_key('visatype'))\n",
    "    \n",
    "    # Conversion of of arrival date into a datetime object\n",
    "    df = df.withColumn(\"arrdate\", get_datetime(df.arrdate))\n",
    "    \n",
    "    # write fact table to Parquet\n",
    "    df.write.parquet(output_data + \"immigration_fact\", mode = \"overwrite\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_fact_df = create_immigration_fact_table(new_immigration_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the relevant data\n",
    "# Operating all function for data cleansing\n",
    "# Creating all defined dimension and fact tables\n",
    "def run_pipeline():"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Basically the quality checks are focus two main areas:\n",
    "\n",
    "1. Does the data schema of the dimensional tables are referring to the data model\n",
    "2. Avoiding empty tables after the ETL was executed\n",
    "\n",
    "Please refer to the respective Jupyter Notebook:\n",
    "[Check Data Quality.ipynb][https://github.com/KCvW/DataEng/blob/main/Capstone/Check%20Data%20Quality.ipynb]\n",
    "\n",
    "A quick check can be performed by executing this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "table_dfs = {\n",
    "    'immigration_fact': immigration_fact_df,\n",
    "    'visa_type_dim': visatype_df,\n",
    "    'calendar_dim': calendar_df,\n",
    "    'usa_demographics_dim': demographics_dim_df,\n",
    "    'country_dim': country_dim_f\n",
    "}\n",
    "for table_name, table_df in table_dfs.item():\n",
    "    etl_functions.quality_checks(table_df, table_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The final `Data Dictionary` is based on the star schema which was initially described in the former chapters. The star schema consists of all raw data sources that were used in this capstone project. \n",
    "\n",
    "These are the fact and dimension tables created by the ETL process:\n",
    "\n",
    "* [Fact Table](https://github.com/KCvW/DataEng/blob/main/Capstone/Fact%20Table.png)\n",
    "Final fact table - includes all necessary features and descriptions\n",
    "\n",
    "* [Country Dimension Table](https://github.com/KCvW/DataEng/blob/main/Capstone/Country%20Dimension%20Table.png)\n",
    "Focussing on `country_code` and `country_name` - these fields were extracted from the SAS file - `average_temperature` is gathered from the global land temperature file\n",
    "\n",
    "* [Visa Type Dimension Table](https://github.com/KCvW/DataEng/blob/main/Capstone/Visa%20Type%20Dimension%20Table.png)\n",
    "Extracts the attributes `visa_type_key` and `visa_type` just to ensure a relibale reconciliation of the data\n",
    "\n",
    "* [Immigration Calendar Dimension Table](https://github.com/KCvW/DataEng/blob/main/Capstone/Immigration%20Calendar%20Dimension%20Table.png)\n",
    "All features were gathered from the initial dataset `immigration`\n",
    "\n",
    "* [US Demographics Dimension Table](https://github.com/KCvW/DataEng/blob/main/Capstone/US%20Demographics%20Dimension%20Table.png)\n",
    "Also this data is based on the a single dataset `us_cities_demographics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
